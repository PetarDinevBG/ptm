{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise will demonstrate how to perform co-occurrence analysis with **R** and the quanteda-package. It is shown how different significance measures can be used to extract semantic links between words. \n",
    "\n",
    "Change to your working directory, create a new R script, load the tm-package and define a few already known default variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(stringsAsFactors = FALSE)\n",
    "require(quanteda)\n",
    "require(magrittr)\n",
    "require(dplyr)\n",
    "require(data.table)\n",
    "\n",
    "# read csv into a data.frame\n",
    "textdata <- read.csv(\"data/data job posts.csv\", header = TRUE, sep = \",\", encoding = \"UTF-8\",quote = \"\\\"\")\n",
    "textdata <- as.data.table(textdata)\n",
    "\n",
    "english_stopwords <- readLines(\"resources/stopwords_en.txt\", encoding = \"UTF-8\")\n",
    "\n",
    "textdata %<>% filter(!duplicated(jobpost))\n",
    "textdata %<>% mutate(d_id = 1:nrow(textdata))\n",
    "\n",
    "#Build a dictionary of lemmas\n",
    "lemmaData <- read.csv2(\"resources/baseform_en.tsv\", sep=\"\\t\", header=FALSE, encoding = \"UTF-8\", stringsAsFactors = F)\n",
    "\n",
    "data_corpus <- corpus(textdata$jobpost, docnames = textdata$d_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence detection\n",
    "\n",
    "The separation of the text into semantic analysis units is important for co-occurrence analysis. Context windows can be for instance documents, paragraphs or sentences or neighboring words. One of the most frequently used context window is the sentence. \n",
    "\n",
    "Documents are decomposed into sentences. Sentences are defined as a separate (quasi-)documents in a new corpus object of the tm-package. The further application of the tm-package functions remains the same. In contrast to previous exercises, however, we now use sentences which are stored as individual documents in the body.\n",
    "\n",
    "Important: The sentence segmentation must take place *before* the other preprocessing steps because the sentence-segmentation-model relies on intact word forms and punctuation marks.\n",
    "\n",
    "The following code defines two functions. One selects documents, the other decomposes a given document text into sentences with the help of a probabilistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "require(openNLP)\n",
    "\n",
    "# Function to convert a document in a vector of sentences\n",
    "convert_text_to_sentences <- function(text, lang = \"en\", SentModel = \"resources/en-sent.bin\") {\n",
    "  \n",
    "  # Calculate sentenve boundaries as annotation with Apache OpenNLP Maxent-sentence-detector.\n",
    "  sentence_token_annotator <- Maxent_Sent_Token_Annotator(language = lang, model = SentModel)\n",
    "  \n",
    "  # Convert text to NLP string\n",
    "  text <- NLP::as.String(text)\n",
    "  \n",
    "  # Annotate the sentence boundaries\n",
    "  sentenceBoundaries <- NLP::annotate(text, sentence_token_annotator)\n",
    "  \n",
    "  # Select sentences as rows of a new matrix\n",
    "  sentences <- text[sentenceBoundaries]\n",
    "  \n",
    "  # return the sentences\n",
    "  return(sentences)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a corpus of documents into a corpus of single sentences from documents\n",
    "reshape_corpus <- function(currentCorpus, ...) {\n",
    "  \n",
    "  # Extraction of all sentences from the corpus as a list\n",
    "  text <- lapply(currentCorpus, as.character)\n",
    "  \n",
    "  # convert the text\n",
    "  pb <- txtProgressBar(min=0, max=length(text))\n",
    "  i <- 0\n",
    "  docs <- lapply(text, FUN=function(x){\n",
    "    i <<- i + 1\n",
    "    setTxtProgressBar(pb, i)\n",
    "    convert_text_to_sentences(x)\n",
    "  }, ...)\n",
    "  close(pb)\n",
    "  \n",
    "  docs <- as.vector(unlist(docs))\n",
    "  \n",
    "  return(docs)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the function `reshape_corpus` on our corpus of job posts to receive a corpus of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original corpus length and its first document\n",
    "length(data_corpus$documents$texts)\n",
    "substr(as.character(data_corpus$documents$texts[1]), 0, 200)\n",
    "# reshape into sentences\n",
    "sentences <- data.table(text=reshape_corpus(data_corpus$documents$texts))\n",
    "\n",
    "sentences %<>% mutate(s_id = 1:nrow(sentences))\n",
    "\n",
    "sentences_corpus <- corpus(sentences$text, docnames = sentences$s_id)\n",
    "\n",
    "save(sentences_corpus,file=\"sentence_corpus.Rdata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing chain\n",
    "\n",
    "#IF JAVA DID NOT WORK AND FOR TIME PURPOSE\n",
    "#load(file=\"sentence_corpus.Rdata\")\n",
    "\n",
    "# reshaped corpus length and its first 'document'\n",
    "length(sentences_corpus$documents$texts)\n",
    "as.character(sentences_corpus$documents$texts[1])\n",
    "as.character(sentences_corpus$documents$texts[2])\n",
    "\n",
    "# Create a DTM (may take a while)\n",
    "data_dfm_entries <- sentences_corpus %>% tokens() %>%\n",
    "  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>% tokens_tolower() %>% \n",
    "  tokens_replace(., lemmaData$V1, lemmaData$V2) %>%\n",
    "  tokens_ngrams(1) %>% tokens_remove(pattern = stopwords()) %>% dfm()\n",
    "\n",
    "\n",
    "data_dfm_entries_sub <- data_dfm_entries %>%\n",
    "  dfm_select(pattern = \"[a-z]\", valuetype = \"regex\", selection = 'keep')\n",
    "\n",
    "colnames(data_dfm_entries_sub) <- colnames(data_dfm_entries_sub) %>% stringi::stri_replace_all_regex(\"[^_a-z]\", \"\") \n",
    "\n",
    "DTM <- dfm_compress(data_dfm_entries_sub, \"features\")\n",
    "# Show some information\n",
    "DTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we create a document-term-matrix. Only word forms which occur at least 10 times should be taken into account. An upper limit is not set (`Inf` = infinite).\n",
    "\n",
    "Additionally, we are interested in the joint occurrence of words in a sentence. For this, we do not need the exact count of how often the terms occur, but only the information whether they occur together or not. This can be encoded in a binary document-term-matrix. The parameter `weighting` in the control options calls the ` weightBin` function. This writes a 1 into the DTM if the term is contained in a sentence and 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimumFrequency <- 10\n",
    "binDTM <- DTM %>% dfm_trim(min_docfreq = minimumFrequency)\n",
    "#Make Binary\n",
    "if (any(binDTM > 1)) {\n",
    "   binDTM <- binDTM >= 1 + 0\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting co-occurrences\n",
    "\n",
    "The counting of the joint word occurrence is easily possible via a matrix multiplication (https://en.wikipedia.org/wiki/Matrix_multiplication) on the binary DTM. For this purpose, the transposed matrix (dimensions: nTypes x nDocs) is multiplied by the original matrix (nDocs x nTypes), which as a result encodes a term-term matrix (dimensions: nTypes x nTypes).\n",
    "\n",
    "However, since we are working on very large matrices and the sparse matrix format (`slam`) which is used by the tm-package does not fully support the matrix multiplication, we first have to convert the `binDTM` into the format of the Matrix package which is more convenient to use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication for cooccurrence counts\n",
    "coocCounts <- t(binDTM) %*% binDTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a snippet of the result. The matrix has `nTerms` rows and columns and is symmetric. Each cell contains the number of joint occurrences. In the diagonal, the frequencies of single occurrences of each term are encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as.matrix(coocCounts[202:205, 202:205])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interprete as follows: *open* appears together *20878* times with *date* in the *236628* sentences of the SUTO addresses. *open* alone occurs *25147* times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical significance\n",
    "\n",
    "In order to not only count joint occurrence we have to determine their significance. Different significance-measures can be used. We need also various counts to calculate the significance of the joint occurrence of a term `i` (`coocTerm`) with any other term `j`:\n",
    "* k - Number of all context units in the corpus\n",
    "* ki - Number of occurrences of `coocTerm`\n",
    "* kj - Number of occurrences of comparison term j\n",
    "* kij - Number of joint occurrences of `coocTerm` and j\n",
    "\n",
    "These quantities can be calculated for any term `coocTerm` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coocTerm <- \"work\"\n",
    "k <- nrow(binDTM)\n",
    "ki <- sum(binDTM[, coocTerm])\n",
    "kj <- colSums(binDTM)\n",
    "names(kj) <- colnames(binDTM)\n",
    "kij <- coocCounts[coocTerm, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation in *R* for Mutual Information, Dice, and Log-Likelihood may look like this. At the end of each formula, the result is sorted so that the most significant co-occurrences are at the first ranks of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## MI: log(k*kij / (ki * kj) ########\n",
    "mutualInformationSig <- log(k * kij / (ki * kj))\n",
    "mutualInformationSig <- mutualInformationSig[order(mutualInformationSig, decreasing = TRUE)]\n",
    "\n",
    "########## DICE: 2 X&Y / X + Y ##############\n",
    "dicesig <- 2 * kij / (ki + kj)\n",
    "dicesig <- dicesig[order(dicesig, decreasing=TRUE)]\n",
    "  \n",
    "########## Log Likelihood ###################\n",
    "logsig <- 2 * ((k * log(k)) - (ki * log(ki)) - (kj * log(kj)) + (kij * log(kij)) \n",
    "          + (k - ki - kj + kij) * log(k - ki - kj + kij) \n",
    "          + (ki - kij) * log(ki - kij) + (kj - kij) * log(kj - kij) \n",
    "          - (k - ki) * log(k - ki) - (k - kj) * log(k - kj))\n",
    "logsig <- logsig[order(logsig, decreasing=T)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the four variants for the statistical extraction of co-occurrence terms is shown in a data frame below. It can be seen that frequency is a bad indicator of meaning constitution. Mutual information emphasizes rather rare events in the data. Dice and Log-likelihood yield very well interpretable contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all significance statistics in one Data-Frame\n",
    "resultOverView <- data.frame(\n",
    "  names(sort(kij, decreasing=T)[1:10]), sort(kij, decreasing=T)[1:10],\n",
    "  names(mutualInformationSig[1:10]), mutualInformationSig[1:10], \n",
    "  names(dicesig[1:10]), dicesig[1:10], \n",
    "  names(logsig[1:10]), logsig[1:10],\n",
    "  row.names = NULL)\n",
    "colnames(resultOverView) <- c(\"Freq-terms\", \"Freq\", \"MI-terms\", \"MI\", \"Dice-Terms\", \"Dice\", \"LL-Terms\", \"LL\")\n",
    "print(resultOverView)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of co-occurrence\n",
    "\n",
    "In the following, we create a network visualization of significant co-occurrences. \n",
    "\n",
    "For this, we provide the calculation of the co-occurrence significance measures, which we have just introduced, as single function in the file `calculateCoocStatistics.R`.  This function can be imported into the current R-Session with the `source` command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the source code for the co-occurrence calculation\n",
    "source(\"calculateCoocStatistics.R\")\n",
    "# Definition of a parameter for the representation of the co-occurrences of a concept\n",
    "numberOfCoocs <- 15\n",
    "# Determination of the term of which co-competitors are to be measured.\n",
    "coocTerm <- \"experience\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the imported function `calculateCoocStatistics` to calculate the co-occurrences for the target term *\"california\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coocs <- calculateCoocStatistics(coocTerm, binDTM, measure=\"LOGLIK\")\n",
    "# Display the numberOfCoocs main terms\n",
    "print(coocs[1:numberOfCoocs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To acquire an extended semantic environment of the target term, 'secondary co-occurrence' terms can be computed for each co-occurrence term of the target term. This results in a graph that can be visualized with special layout algorithms (e.g. Force Directed Graph). \n",
    "\n",
    "Network graphs can be evaluated and visualized in R with the `igraph`-package. Any graph object can be created from a three-column data-frame. Each row in that data-frame is a triple. Each triple encodes an edge-information of two nodes (source, sink) and an edge-weight value. \n",
    "\n",
    "For a term co-occurrence network, each triple consists of the target word, a co-occurring word and the significance of their joint occurrence. We denote the values with *from, to, sig*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of gathering the network for the target term runs in two steps. First, we obtain all significant co-occurrence terms for the target term. Second, we obtain all co-occurrences of the co-occurrence terms from step one.\n",
    "\n",
    "Intermediate results for each term are stored as temporary triples named `tmpGraph`. With the `rbind` command (\"row bind\", used for concatenation of data-frames) all `tmpGraph` are appended to the complete network object stored in `resultGraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The structure of the temporary graph object is equal to that of the resultGraph\n",
    "tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))\n",
    "\n",
    "# Fill the data.frame to produce the correct number of lines\n",
    "tmpGraph[1:numberOfCoocs, 3] <- coocs[1:numberOfCoocs]\n",
    "# Entry of the search word into the first column in all lines\n",
    "tmpGraph[, 1] <- coocTerm\n",
    "# Entry of the co-occurrences into the second column of the respective line\n",
    "tmpGraph[, 2] <- names(coocs)[1:numberOfCoocs]\n",
    "# Set the significances\n",
    "tmpGraph[, 3] <- coocs[1:numberOfCoocs]\n",
    "\n",
    "# Attach the triples to resultGraph\n",
    "resultGraph <- rbind(resultGraph, tmpGraph)\n",
    "\n",
    "# Iteration over the most significant numberOfCoocs co-occurrences of the search term\n",
    "for (i in 1:numberOfCoocs){\n",
    "  \n",
    "  # Calling up the co-occurrence calculation for term i from the search words co-occurrences\n",
    "  newCoocTerm <- names(coocs)[i]\n",
    "  coocs2 <- calculateCoocStatistics(newCoocTerm, binDTM, measure=\"LOGLIK\")\n",
    "  \n",
    "  # print the co-occurrences\n",
    "  coocs2[1:10]\n",
    "  \n",
    "  # Structure of the temporary graph object\n",
    "  tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))\n",
    "  tmpGraph[1:numberOfCoocs, 3] <- coocs2[1:numberOfCoocs]\n",
    "  tmpGraph[, 1] <- newCoocTerm\n",
    "  tmpGraph[, 2] <- names(coocs2)[1:numberOfCoocs]\n",
    "  tmpGraph[, 3] <- coocs2[1:numberOfCoocs]\n",
    "  \n",
    "  #Append the result to the result graph\n",
    "  resultGraph <- rbind(resultGraph, tmpGraph[2:length(tmpGraph[, 1]), ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, `resultGraph` now contains all `numberOfCoocs * numberOfCoocs` edges of a term co-occurrence network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of some examples from resultGraph\n",
    "resultGraph[sample(nrow(resultGraph), 6), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package iGraph offers multiple graph visualizations for graph objects. Graph objects can be created from triple lists, such as those we just generated. In the next step we load the package iGraph and create a visualization of all nodes and edges from the object `resultGraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "require(igraph)\n",
    "\n",
    "# Set the graph and type. In this case, \"F\" means \"Force Directed\"\n",
    "graphNetwork <- graph.data.frame(resultGraph, directed = F)\n",
    "\n",
    "# Identification of all nodes with less than 2 edges\n",
    "graphVs <- V(graphNetwork)[degree(graphNetwork) < 2]\n",
    "# These edges are removed from the graph\n",
    "graphNetwork <- delete.vertices(graphNetwork, graphVs) \n",
    "\n",
    "# Assign colors to edges and nodes (searchterm blue, rest orange)\n",
    "V(graphNetwork)$color <- ifelse(V(graphNetwork)$name == coocTerm, 'cornflowerblue', 'orange') \n",
    "\n",
    "# Edges with a significance of at least 50% of the maximum sig- nificance in the graph are drawn in orange\n",
    "halfMaxSig <- max(E(graphNetwork)$sig) * 0.5\n",
    "E(graphNetwork)$color <- ifelse(E(graphNetwork)$sig > halfMaxSig, \"coral\", \"azure3\")\n",
    "\n",
    "# Disable edges with radius\n",
    "E(graphNetwork)$curved <- 0 \n",
    "# Size the nodes by their degree of networking\n",
    "V(graphNetwork)$size <- log(degree(graphNetwork)) * 5\n",
    "\n",
    "# All nodes must be assigned a standard minimum-size\n",
    "V(graphNetwork)$size[V(graphNetwork)$size < 5] <- 3 \n",
    "\n",
    "# edge thickness\n",
    "E(graphNetwork)$width <- 2\n",
    "\n",
    "# Define the frame and spacing for the plot\n",
    "par(mai=c(0,0,1,0)) \n",
    "\n",
    "# Finaler Plot\n",
    "plot(graphNetwork,\t\t\t\t\n",
    "     layout = layout.fruchterman.reingold,\t# Force Directed Layout \n",
    "     main = paste(coocTerm, ' Graph'),\n",
    "     vertex.label.family = \"sans\",\n",
    "     vertex.label.cex = 0.8,\n",
    "     vertex.shape = \"circle\",\n",
    "     vertex.label.dist = 0.5,\t\t\t# Labels of the nodes moved slightly\n",
    "     vertex.frame.color = 'darkolivegreen',\n",
    "     vertex.label.color = 'black',\t\t# Color of node names\n",
    "     vertex.label.font = 2,\t\t\t# Font of node names\n",
    "     vertex.label = V(graphNetwork)$name,\t\t# node names\n",
    "     vertex.label.cex = 1 # font size of node names \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
